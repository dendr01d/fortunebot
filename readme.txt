This is a project to make a twitter bot that uses natural language generation to create "fortunes" like the kind that you'd find in a fortune cookie.

main.py is the driver of the program
It authorizes itself with twitter, builds the model from the corpus, then runs the main loop:
-check if enough time has passed to tweet. If it has:
	-spit out a new tweet generated by the model
	-figure out how long to wait until tweeting again

stt.py implements a State Transition Table
Basically it can be thought of as representing a finite-state automaton
This forms the backbone of the proceding models, which differ mainly in how they actually interact with the STT

markov.py implements a regular Markov Chain
It maps singular words to singular words and generates snippets one word at a time

markov2.py implements a slightly more sophisticated model called an Additive Markov Chain
It maps pairs of words to singular words. It generates snippets one word at a time, but based on the last TWO preceding words instead of one
It can produce more comprehensible snippets, but it needs a quadratically-scaled corpus to maintain the level of connectivity of an ordinary Markov Chain
To deal with the corpus problem I've started working on a method of generation with built-in mutation in tandem with a training suite for quickly generating and reinforcing snippets.

test_* scripts are for testing the code in the corresponding files

util.py implements some functions for manipulating text

training.py implements a suite for training and saving a model


Stuff I may want to implement later:
- let people tweet pictures of fortunes to the bot, then have it read them and add them to the corpus
- reinforcement learning based on which tweets people like or retweet
- beam-search based algorithm. Instead of performing a single random walk through the graph, generate a small collection of snippets and then judge the best one based on some probabilistic criteria
